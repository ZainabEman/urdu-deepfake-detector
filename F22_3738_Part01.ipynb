{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/datasets/CSALT/deepfake_detection_dataset_urdu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47xb2CMWdpTl",
        "outputId": "f387856a-87df-4e5f-eabd-43444738a9c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deepfake_detection_dataset_urdu'...\n",
            "remote: Enumerating objects: 6796, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 6796 (delta 0), reused 0 (delta 0), pack-reused 6793 (from 1)\u001b[K\n",
            "Receiving objects: 100% (6796/6796), 957.64 KiB | 4.37 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "Updating files: 100% (6796/6796), done.\n",
            "Filtering content: 100% (6794/6794), 1.82 GiB | 38.73 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "\n",
        "# Parameters\n",
        "SAMPLE_RATE = 16000  # Standard sample rate for audio\n",
        "N_MFCC = 13          # Number of MFCC coefficients\n",
        "MAX_FRAMES = 300     # Fixed number of frames for uniform input\n",
        "N_SAMPLES_PER_CLASS = 500  # Number of samples per class (Bonafide and Deepfake)\n",
        "\n",
        "# Function to extract MFCC features\n",
        "def extract_mfcc(file_path):\n",
        "    \"\"\"Extract MFCC features from an audio file\"\"\"\n",
        "    try:\n",
        "        # Load audio file\n",
        "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "\n",
        "        # Extract MFCCs\n",
        "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC)\n",
        "\n",
        "        # Pad or truncate to MAX_FRAMES\n",
        "        if mfcc.shape[1] > MAX_FRAMES:\n",
        "            mfcc = mfcc[:, :MAX_FRAMES]\n",
        "        else:\n",
        "            mfcc = np.pad(mfcc, ((0, 0), (0, MAX_FRAMES - mfcc.shape[1])), mode='constant')\n",
        "\n",
        "        # Flatten to 1D vector\n",
        "        mfcc_flat = mfcc.flatten()\n",
        "        return mfcc_flat\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to collect file paths recursively\n",
        "def collect_file_paths(dataset_dir):\n",
        "    \"\"\"Collect file paths and labels from dataset recursively\"\"\"\n",
        "    bonafide_files = []\n",
        "    deepfake_files = []\n",
        "\n",
        "    # Check for Bonafide folder\n",
        "    bonafide_dir = None\n",
        "    for possible_name in ['Bonafide', 'bonafide', 'Bonafide_Audio', 'Real']:\n",
        "        candidate = os.path.join(dataset_dir, possible_name)\n",
        "        if os.path.isdir(candidate):\n",
        "            bonafide_dir = candidate\n",
        "            break\n",
        "\n",
        "    if bonafide_dir is None:\n",
        "        print(\"Error: Could not find Bonafide folder in\", dataset_dir)\n",
        "        print(\"Checked for: Bonafide, bonafide, Bonafide_Audio, Real\")\n",
        "        return [], []\n",
        "\n",
        "    # Bonafide files (recursive search)\n",
        "    print(f\"Scanning Bonafide folder: {bonafide_dir}\")\n",
        "    for root, _, files in os.walk(bonafide_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.wav'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                bonafide_files.append(file_path)\n",
        "\n",
        "    # Deepfake files (Spoofed_Tacotron and Spoofed_TTS)\n",
        "    for spoof_dir_name in ['Spoofed_Tacotron', 'spoofed_tacotron', 'Spoofed_TTS', 'spoofed_tts']:\n",
        "        spoof_dir = os.path.join(dataset_dir, spoof_dir_name)\n",
        "        if os.path.isdir(spoof_dir):\n",
        "            print(f\"Scanning Deepfake folder: {spoof_dir}\")\n",
        "            for root, _, files in os.walk(spoof_dir):\n",
        "                for file in files:\n",
        "                    if file.lower().endswith('.wav'):\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        deepfake_files.append(file_path)\n",
        "\n",
        "    return bonafide_files, deepfake_files\n",
        "\n",
        "# Main preprocessing function\n",
        "def preprocess_dataset(dataset_dir, output_dir='preprocessed'):\n",
        "    \"\"\"Preprocess audio dataset and save features\"\"\"\n",
        "    # Create output directory\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Collect file paths\n",
        "    bonafide_files, deepfake_files = collect_file_paths(dataset_dir)\n",
        "    print(f\"Found {len(bonafide_files)} Bonafide files and {len(deepfake_files)} Deepfake files\")\n",
        "\n",
        "    # Validate dataset\n",
        "    if len(bonafide_files) == 0:\n",
        "        raise ValueError(\"No Bonafide files found. Cannot proceed with preprocessing.\")\n",
        "    if len(deepfake_files) == 0:\n",
        "        raise ValueError(\"No Deepfake files found. Cannot proceed with preprocessing.\")\n",
        "\n",
        "    # Balance dataset\n",
        "    random.seed(42)  # For reproducibility\n",
        "    max_samples = min(N_SAMPLES_PER_CLASS, len(bonafide_files), len(deepfake_files))\n",
        "    if max_samples < N_SAMPLES_PER_CLASS:\n",
        "        print(f\"Warning: Reducing samples per class to {max_samples} due to limited files\")\n",
        "\n",
        "    bonafide_files = random.sample(bonafide_files, max_samples)\n",
        "    deepfake_files = random.sample(deepfake_files, max_samples)\n",
        "\n",
        "    # Initialize arrays\n",
        "    X = []\n",
        "    y = []\n",
        "    file_list = []\n",
        "\n",
        "    # Process Bonafide files (label = 0)\n",
        "    print(\"Processing Bonafide files...\")\n",
        "    for file_path in bonafide_files:\n",
        "        mfcc = extract_mfcc(file_path)\n",
        "        if mfcc is not None:\n",
        "            X.append(mfcc)\n",
        "            y.append(0)\n",
        "            file_list.append({'file_path': file_path, 'label': 'Bonafide'})\n",
        "\n",
        "    # Process Deepfake files (label = 1)\n",
        "    print(\"Processing Deepfake files...\")\n",
        "    for file_path in deepfake_files:\n",
        "        mfcc = extract_mfcc(file_path)\n",
        "        if mfcc is not None:\n",
        "            X.append(mfcc)\n",
        "            y.append(1)\n",
        "            file_list.append({'file_path': file_path, 'label': 'Deepfake'})\n",
        "\n",
        "    # Convert to NumPy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Validate output\n",
        "    if len(X) == 0:\n",
        "        raise ValueError(\"No valid features extracted. Check audio files for errors.\")\n",
        "\n",
        "    # Shuffle data\n",
        "    X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "    # Save features and labels\n",
        "    np.save(os.path.join(output_dir, 'X.npy'), X)\n",
        "    np.save(os.path.join(output_dir, 'y.npy'), y)\n",
        "\n",
        "    # Save file list\n",
        "    file_df = pd.DataFrame(file_list)\n",
        "    file_df.to_csv(os.path.join(output_dir, 'file_list.csv'), index=False)\n",
        "\n",
        "    print(f\"Preprocessed {len(X)} samples with {X.shape[1]} features each\")\n",
        "    print(f\"Saved features to {output_dir}/X.npy and labels to {output_dir}/y.npy\")\n",
        "    print(f\"Saved file list to {output_dir}/file_list.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_dir = \"/content/deepfake_detection_dataset_urdu\"  # Updated path\n",
        "    preprocess_dataset(dataset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBshU22W_0ic",
        "outputId": "9cc2a7f4-c162-4d27-cdfb-eaa1eb2d8b40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning Bonafide folder: /content/deepfake_detection_dataset_urdu/Bonafide\n",
            "Scanning Deepfake folder: /content/deepfake_detection_dataset_urdu/Spoofed_Tacotron\n",
            "Scanning Deepfake folder: /content/deepfake_detection_dataset_urdu/Spoofed_TTS\n",
            "Found 3398 Bonafide files and 3396 Deepfake files\n",
            "Processing Bonafide files...\n",
            "Processing Deepfake files...\n",
            "Preprocessed 1000 samples with 3900 features each\n",
            "Saved features to preprocessed/X.npy and labels to preprocessed/y.npy\n",
            "Saved file list to preprocessed/file_list.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y = np.load('preprocessed/y.npy')\n",
        "print(f\"Bonafide samples (label 0): {sum(y == 0)}\")\n",
        "print(f\"Deepfake samples (label 1): {sum(y == 1)}\")"
      ],
      "metadata": {
        "id": "_HPf7_VuGCAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "845b16db-7d34-4efc-8319-9b6504199430"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonafide samples (label 0): 500\n",
            "Deepfake samples (label 1): 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Load preprocessed data\n",
        "X = np.load('preprocessed/X.npy')\n",
        "y = np.load('preprocessed/y.npy')\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize models\n",
        "svm_model = SVC(probability=True, random_state=42)\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "perceptron_model = Perceptron(random_state=42)\n",
        "dnn_model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)\n",
        "\n",
        "# Train models\n",
        "print(\"Training SVM...\")\n",
        "svm_model.fit(X_train, y_train)\n",
        "print(\"Training Logistic Regression...\")\n",
        "lr_model.fit(X_train, y_train)\n",
        "print(\"Training Perceptron...\")\n",
        "perceptron_model.fit(X_train, y_train)\n",
        "print(\"Training DNN...\")\n",
        "dnn_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "models = {'SVM': svm_model, 'Logistic Regression': lr_model, 'Perceptron': perceptron_model, 'DNN': dnn_model}\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else (y_pred).astype(float)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc_roc = roc_auc_score(y_test, y_prob) if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    if auc_roc is not None:\n",
        "        print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
        "    else:\n",
        "        print(\"AUC-ROC: Not available (model does not support probabilities)\")\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Save models\n",
        "joblib.dump(svm_model, 'models/svm_model.pkl')\n",
        "joblib.dump(lr_model, 'models/lr_model.pkl')\n",
        "joblib.dump(perceptron_model, 'models/perceptron_model.pkl')\n",
        "joblib.dump(dnn_model, 'models/dnn_model.pkl')\n",
        "print(\"\\nModels saved to 'models/' directory\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jGE5yPZVKSp",
        "outputId": "07aad848-5ee8-4508-ecb2-1fe68a601145"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM...\n",
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perceptron...\n",
            "Training DNN...\n",
            "\n",
            "SVM Results:\n",
            "Accuracy: 0.8050\n",
            "Precision: 0.8144\n",
            "Recall: 0.7900\n",
            "F1-Score: 0.8020\n",
            "AUC-ROC: 0.9079\n",
            "\n",
            "Logistic Regression Results:\n",
            "Accuracy: 0.7950\n",
            "Precision: 0.7980\n",
            "Recall: 0.7900\n",
            "F1-Score: 0.7940\n",
            "AUC-ROC: 0.8712\n",
            "\n",
            "Perceptron Results:\n",
            "Accuracy: 0.7350\n",
            "Precision: 0.6942\n",
            "Recall: 0.8400\n",
            "F1-Score: 0.7602\n",
            "AUC-ROC: Not available (model does not support probabilities)\n",
            "\n",
            "DNN Results:\n",
            "Accuracy: 0.7550\n",
            "Precision: 0.7802\n",
            "Recall: 0.7100\n",
            "F1-Score: 0.7435\n",
            "AUC-ROC: 0.8546\n",
            "\n",
            "Models saved to 'models/' directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load preprocessed data\n",
        "X = np.load('preprocessed/X.npy')\n",
        "y = np.load('preprocessed/y.npy')\n",
        "\n",
        "# Split into training and test sets (same split as training)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Load models\n",
        "svm_model = joblib.load('models/svm_model.pkl')\n",
        "lr_model = joblib.load('models/lr_model.pkl')\n",
        "perceptron_model = joblib.load('models/perceptron_model.pkl')\n",
        "dnn_model = joblib.load('models/dnn_model.pkl')\n",
        "\n",
        "# Evaluate models\n",
        "models = {'SVM': svm_model, 'Logistic Regression': lr_model, 'Perceptron': perceptron_model, 'DNN': dnn_model}\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc_roc = roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
        "\n",
        "    # Append results\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'AUC-ROC': auc_roc if auc_roc is not None else 'N/A'\n",
        "    })\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Bonafide', 'Deepfake'], yticklabels=['Bonafide', 'Deepfake'])\n",
        "    plt.title(f'Confusion Matrix - {name}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.savefig(f'confusion_matrix_{name.lower().replace(\" \", \"_\")}.png')\n",
        "    plt.close()\n",
        "\n",
        "# Create comparison table\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nModel Comparison Table:\")\n",
        "print(results_df.to_string(index=False))\n",
        "results_df.to_csv('model_comparison.csv', index=False)\n",
        "\n",
        "# Plot comparison bar chart\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "x = np.arange(len(models))\n",
        "width = 0.2\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax.bar(x + i * width, results_df[metric], width, label=metric)\n",
        "\n",
        "ax.set_xticks(x + width * 1.5)\n",
        "ax.set_xticklabels(models.keys())\n",
        "ax.set_title('Model Performance Comparison')\n",
        "ax.set_ylabel('Score')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison_bar_chart.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nConfusion matrices saved as PNG files.\")\n",
        "print(\"Comparison table saved to 'model_comparison.csv'\")\n",
        "print(\"Comparison bar chart saved to 'model_comparison_bar_chart.png'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49KeGbSeVmn7",
        "outputId": "41e57163-4dcb-48f7-d57b-9d690fa75774"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Comparison Table:\n",
            "              Model  Accuracy  Precision  Recall  F1-Score  AUC-ROC\n",
            "                SVM     0.805   0.814433    0.79  0.802030   0.9079\n",
            "Logistic Regression     0.795   0.797980    0.79  0.793970   0.8712\n",
            "         Perceptron     0.735   0.694215    0.84  0.760181      N/A\n",
            "                DNN     0.755   0.780220    0.71  0.743455  0.85455\n",
            "\n",
            "Confusion matrices saved as PNG files.\n",
            "Comparison table saved to 'model_comparison.csv'\n",
            "Comparison bar chart saved to 'model_comparison_bar_chart.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import librosa\n",
        "import joblib\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "N_MFCC = 13\n",
        "MAX_FRAMES = 300\n",
        "\n",
        "def extract_mfcc(path):\n",
        "    audio, _ = librosa.load(path, sr=SAMPLE_RATE)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=N_MFCC)\n",
        "    if mfcc.shape[1] > MAX_FRAMES:\n",
        "        mfcc = mfcc[:, :MAX_FRAMES]\n",
        "    else:\n",
        "        mfcc = np.pad(mfcc, ((0,0),(0,MAX_FRAMES-mfcc.shape[1])), mode='constant')\n",
        "    return mfcc.flatten()\n",
        "\n",
        "def predict_audio(audio_path, model_choice):\n",
        "    model_paths = {\n",
        "        \"SVM\": \"/content/models/svm_model.pkl\",\n",
        "        \"Logistic Regression\": \"/content/models/lr_model.pkl\",\n",
        "        \"DNN\": \"/content/models/dnn_model.pkl\",\n",
        "        \"Perceptron\": \"/content/models/perceptron_model.pkl\"\n",
        "    }\n",
        "    model = joblib.load(model_paths[model_choice])\n",
        "    feats = extract_mfcc(audio_path).reshape(1, -1)\n",
        "    pred = model.predict(feats)[0]\n",
        "    label = \"Bonafide\" if pred == 0 else \"Deepfake\"\n",
        "    conf = None\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        conf = float(np.max(model.predict_proba(feats)))\n",
        "    return label, f\"{conf:.2%}\" if conf is not None else \"N/A\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Zainab's Deep Fake detector\")\n",
        "    with gr.Row():\n",
        "        # <-- removed source=\"upload\"\n",
        "        audio_in = gr.Audio(label=\"Upload .wav/.mp3\", type=\"filepath\")\n",
        "        model_sel = gr.Radio([\"SVM\", \"Logistic Regression\", \"DNN\", \"Perceptron\"],\n",
        "                             value=\"SVM\", label=\"Model\")\n",
        "    btn = gr.Button(\"Analyze\")\n",
        "    out_label = gr.Label(label=\"Prediction\")\n",
        "    out_conf  = gr.Textbox(label=\"Confidence\", interactive=False)\n",
        "    btn.click(predict_audio, [audio_in, model_sel], [out_label, out_conf])\n",
        "\n",
        "# share=True gives you a public URL from Colab\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "qzt_-OFAXFc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "540bd6fd-9cee-4e36-f9f5-6ab3f8dd39f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://71e5f627ce619c0122.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://71e5f627ce619c0122.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}